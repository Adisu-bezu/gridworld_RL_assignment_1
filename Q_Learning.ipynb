{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca146096-1bd5-416e-b7b4-36f9270b051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-values:\n",
      "State (0, 0): {'U': -1.3906558000000007, 'D': -0.4340620000000006, 'L': -1.3906558000000007, 'R': -0.4340620000000006}\n",
      "State (0, 1): {'U': -0.4340620000000094, 'D': 0.6288199999999993, 'L': -1.3906558000000007, 'R': 0.6288199999999993}\n",
      "State (0, 2): {'U': 0.6288199999926539, 'D': 1.8097999999938381, 'L': -0.43406200000001105, 'R': 1.8097999999999992}\n",
      "State (0, 3): {'U': 1.8097999999900245, 'D': 3.121999999999999, 'L': 0.628819999999326, 'R': 3.121999999484834}\n",
      "State (0, 4): {'U': 3.1211145877283344, 'D': 4.579999999998473, 'L': 1.8097999977283479, 'R': 3.1219999587323155}\n",
      "State (1, 0): {'U': -1.3906558000000007, 'D': 0.6288199999999993, 'L': -0.4340620000000006, 'R': 0.6288199999999993}\n",
      "State (1, 1): {'U': -0.4340620000000006, 'D': 1.8097999999999992, 'L': -0.4340620000000006, 'R': 1.8097999999999992}\n",
      "State (1, 2): {'U': 0.6288199999999974, 'D': 3.121999999999999, 'L': 0.6288199999999993, 'R': 3.121999999999999}\n",
      "State (1, 3): {'U': 1.8097999999999983, 'D': 4.579999999999998, 'L': 1.8097999999999992, 'R': 4.579999999999968}\n",
      "State (1, 4): {'U': 3.121999998709068, 'D': 6.199999999999999, 'L': 3.121999999992979, 'R': 4.579999999998236}\n",
      "State (2, 0): {'U': -0.4340620000000006, 'D': 1.8097999999999992, 'L': 0.6288199999999993, 'R': 1.8097999999999992}\n",
      "State (2, 1): {'U': 0.6288199999999993, 'D': 3.121999999999999, 'L': 0.6288199999999993, 'R': 3.121999999999999}\n",
      "State (2, 2): {'U': 1.8097999999999992, 'D': 4.579999999999998, 'L': 1.8097999999999992, 'R': 4.579999999999998}\n",
      "State (2, 3): {'U': 3.121999999999999, 'D': 6.199999999999999, 'L': 3.121999999999999, 'R': 6.199999999999999}\n",
      "State (2, 4): {'U': 4.579999999999858, 'D': 8.0, 'L': 4.579999999991266, 'R': 6.199999999999967}\n",
      "State (3, 0): {'U': 0.6288199999999993, 'D': 3.121999999999999, 'L': 1.8097999999999992, 'R': 3.121999999999999}\n",
      "State (3, 1): {'U': 1.8097999999999992, 'D': 4.579999999999998, 'L': 1.8097999999999992, 'R': 4.579999999999998}\n",
      "State (3, 2): {'U': 3.121999999999999, 'D': 6.199999999999999, 'L': 3.121999999999999, 'R': 6.199999999999999}\n",
      "State (3, 3): {'U': 4.579999999999998, 'D': 8.0, 'L': 4.579999999999998, 'R': 8.0}\n",
      "State (3, 4): {'U': 6.199999999967181, 'D': 10.0, 'L': 6.199999999999999, 'R': 7.999999999998934}\n",
      "State (4, 0): {'U': 1.8097999999999992, 'D': 3.1219999999999986, 'L': 3.121999999998425, 'R': 4.579999999999998}\n",
      "State (4, 1): {'U': 3.121999999999999, 'D': 4.579999999999998, 'L': 3.121999999999999, 'R': 6.199999999999999}\n",
      "State (4, 2): {'U': 4.579999999999998, 'D': 6.199999999999999, 'L': 4.579999999999998, 'R': 8.0}\n",
      "State (4, 3): {'U': 6.199999999999999, 'D': 8.0, 'L': 6.199999999999999, 'R': 10.0}\n",
      "State (4, 4): {'U': 0.0, 'D': 0.0, 'L': 0.0, 'R': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the grid world environment\n",
    "GRID_SIZE = 5\n",
    "REWARD_GOAL = 10\n",
    "REWARD_STEP = -1\n",
    "EPISODES = 1000  # Number of training episodes\n",
    "ALPHA = 0.5  # Learning rate\n",
    "GAMMA = 0.9  # Discount factor\n",
    "\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # Up, Down, Left, Right\n",
    "action_names = ['U', 'D', 'L', 'R']\n",
    "num_actions = len(actions)\n",
    "\n",
    "def is_valid(state):\n",
    "    \"\"\"Check if the state is within the grid boundaries.\"\"\"\n",
    "    x, y = state\n",
    "    return 0 <= x < GRID_SIZE and 0 <= y < GRID_SIZE\n",
    "\n",
    "def get_next_state(state, action):\n",
    "    \"\"\"Get the next state given the current state and action.\"\"\"\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    if is_valid(next_state):\n",
    "        return next_state\n",
    "    return state  # If the action leads out of bounds, remain in the same state\n",
    "\n",
    "# Initialize rewards and Q-table\n",
    "rewards = np.full((GRID_SIZE, GRID_SIZE), REWARD_STEP)\n",
    "rewards[GRID_SIZE - 1, GRID_SIZE - 1] = REWARD_GOAL  # Goal state\n",
    "\n",
    "Q = np.zeros((GRID_SIZE, GRID_SIZE, num_actions))  # Q-table\n",
    "\n",
    "def choose_action(state, epsilon):\n",
    "    \"\"\"Epsilon-greedy policy for action selection.\"\"\"\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, num_actions - 1)  # Explore\n",
    "    else:\n",
    "        x, y = state\n",
    "        return np.argmax(Q[x, y])  # Exploit\n",
    "\n",
    "# Q-learning algorithm\n",
    "def q_learning():\n",
    "    for episode in range(EPISODES):\n",
    "        state = (0, 0)  # Start state\n",
    "        epsilon = max(0.1, 1 - episode / (EPISODES / 2))  # Decreasing epsilon\n",
    "\n",
    "        while state != (GRID_SIZE - 1, GRID_SIZE - 1):\n",
    "            x, y = state\n",
    "\n",
    "            # Choose action using epsilon-greedy policy\n",
    "            action_index = choose_action(state, epsilon)\n",
    "            action = actions[action_index]\n",
    "\n",
    "            # Get next state and reward\n",
    "            next_state = get_next_state(state, action)\n",
    "            nx, ny = next_state\n",
    "            reward = rewards[nx, ny]\n",
    "\n",
    "            # Update Q-value\n",
    "            Q[x, y, action_index] += ALPHA * (\n",
    "                reward + GAMMA * np.max(Q[nx, ny]) - Q[x, y, action_index]\n",
    "            )\n",
    "\n",
    "            # Transition to the next state\n",
    "            state = next_state\n",
    "\n",
    "def print_q_values():\n",
    "    \"\"\"Print the learned Q-values for each state and action.\"\"\"\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            print(f\"State ({x}, {y}):\", {action_names[i]: Q[x, y, i] for i in range(num_actions)})\n",
    "\n",
    "# Train the agent and display the results\n",
    "q_learning()\n",
    "print(\"Learned Q-values:\")\n",
    "print_q_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b62d64-c2c4-4570-b02c-b82097e785fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

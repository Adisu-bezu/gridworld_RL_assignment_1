{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f510aa-425e-499c-aca4-a66b9830bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "D D D D D\n",
      "D D D D D\n",
      "D D D D D\n",
      "D D D D D\n",
      "R R R R \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid world environment\n",
    "GRID_SIZE = 5\n",
    "REWARD_GOAL = 10\n",
    "REWARD_STEP = -1\n",
    "DISCOUNT_FACTOR = 0.9\n",
    "THRESHOLD = 1e-4  # Convergence threshold\n",
    "\n",
    "# Actions: up, down, left, right\n",
    "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "action_names = ['U', 'D', 'L', 'R']\n",
    "\n",
    "def is_valid(state):\n",
    "    \"\"\"Check if the state is within the grid boundaries.\"\"\"\n",
    "    x, y = state\n",
    "    return 0 <= x < GRID_SIZE and 0 <= y < GRID_SIZE\n",
    "\n",
    "# Initialize rewards and value function\n",
    "rewards = np.full((GRID_SIZE, GRID_SIZE), REWARD_STEP)\n",
    "rewards[GRID_SIZE - 1, GRID_SIZE - 1] = REWARD_GOAL  # Goal state\n",
    "\n",
    "values = np.zeros((GRID_SIZE, GRID_SIZE))  # Value function\n",
    "policy = np.full((GRID_SIZE, GRID_SIZE), '', dtype='<U1')  # Optimal policy\n",
    "\n",
    "def value_iteration():\n",
    "    \"\"\"Perform value iteration to find the optimal policy.\"\"\"\n",
    "    global values, policy\n",
    "\n",
    "    while True:\n",
    "        delta = 0  # To track convergence\n",
    "        new_values = np.copy(values)\n",
    "\n",
    "        for x in range(GRID_SIZE):\n",
    "            for y in range(GRID_SIZE):\n",
    "                if (x, y) == (GRID_SIZE - 1, GRID_SIZE - 1):\n",
    "                    # Skip the goal state\n",
    "                    continue\n",
    "\n",
    "                best_value = float('-inf')\n",
    "                best_action = ''\n",
    "\n",
    "                for action, name in zip(actions, action_names):\n",
    "                    next_state = (x + action[0], y + action[1])\n",
    "\n",
    "                    if is_valid(next_state):\n",
    "                        nx, ny = next_state\n",
    "                        reward = rewards[nx, ny]\n",
    "                        value = reward + DISCOUNT_FACTOR * values[nx, ny]\n",
    "\n",
    "                        if value > best_value:\n",
    "                            best_value = value\n",
    "                            best_action = name\n",
    "\n",
    "                new_values[x, y] = best_value\n",
    "                policy[x, y] = best_action\n",
    "\n",
    "                delta = max(delta, abs(new_values[x, y] - values[x, y]))\n",
    "\n",
    "        values = new_values\n",
    "\n",
    "        if delta < THRESHOLD:\n",
    "            break\n",
    "\n",
    "def print_policy():\n",
    "    \"\"\"Print the optimal policy for each state.\"\"\"\n",
    "    for row in policy:\n",
    "        print(' '.join(row))\n",
    "\n",
    "# Run value iteration and print the results\n",
    "value_iteration()\n",
    "print(\"Optimal Policy:\")\n",
    "print_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e6048-650a-4432-8269-f2dacaecdc13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
